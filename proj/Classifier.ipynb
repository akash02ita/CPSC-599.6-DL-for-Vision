{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNo14L/0LcxZIrkCjfDuoGI",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/akash02ita/CPSC-599.6-DL-for-Vision/blob/proj/proj/Classifier.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KCq_kESAEg5M",
        "outputId": "3467c5d5-6e22-41c7-ac8f-00e63ff76b44"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'myrepo'...\n",
            "remote: Enumerating objects: 89, done.\u001b[K\n",
            "remote: Counting objects: 100% (89/89), done.\u001b[K\n",
            "remote: Compressing objects: 100% (71/71), done.\u001b[K\n",
            "remote: Total 89 (delta 10), reused 23 (delta 0), pack-reused 0\u001b[K\n",
            "Unpacking objects: 100% (89/89), 15.09 MiB | 5.35 MiB/s, done.\n"
          ]
        }
      ],
      "source": [
        "!git clone --branch proj https://github.com/akash02ita/CPSC-599.6-DL-for-Vision.git myrepo\n",
        "!cp myrepo/proj/CustomDataset.ipynb ."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "__BACKUP__NAME__ = __name__\n",
        "__name__ = \"NOT MAIN\" # import jupyter methods without running test scripts\n",
        "%run CustomDataset.ipynb\n",
        "__name__ = __BACKUP__NAME__"
      ],
      "metadata": {
        "id": "bJXOqi8_Fbif"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Setup Custom Dataset"
      ],
      "metadata": {
        "id": "2BXOv_nxNfkB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "import shutil, zipfile\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "  # Mount the folder\n",
        "  drive.mount('/content/drive', force_remount=True)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JvcJJbdMOK0Z",
        "outputId": "9e7d3187-2f92-49c7-f175-b81866627317"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Setup handwritten text database"
      ],
      "metadata": {
        "id": "NEQn28m8NXRX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "DATASET_PATH = \"/content/drive/MyDrive/599 Project/custom-dataset-script\"\n",
        "\n",
        "handwritten_zip_path = \"/content/drive/MyDrive/599 Project/custom-dataset-script/handwritten-text-images-database.zip\"\n",
        "extract_zip_to(handwritten_zip_path, \".\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "EKqmQ5-oUwBj",
        "outputId": "840cd127-2b71-47e8-afee-146c4b7e81e2"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'.'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 'en_test' folder not present.\n",
        "# train_handwritten = TextDataset(\"database/en_train_filtered\", \"database/en_train_filtered/labels.csv\")\n",
        "# val_handwritten = TextDataset(\"database/en_val\", \"database/en_val/labels.csv\")\n",
        "\n",
        "# split the texts into training, validation and test (60, 20, 20)\n",
        "dataset = TextDataset(\"database/en_train_filtered\", \"database/en_train_filtered/labels.csv\")\n",
        "from torch.utils.data import random_split\n",
        "\n",
        "train_size = int(0.6 * len(dataset))\n",
        "val_size = int(0.2 * len(dataset))\n",
        "test_size = len(dataset) - train_size - val_size\n",
        "\n",
        "train_handwritten, val_handwritten, test_handwritten = random_split(dataset, [train_size, val_size, test_size])\n",
        "print(len(train_handwritten),len(val_handwritten),len(test_handwritten))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3UH1rnFLNWMu",
        "outputId": "febf305a-846b-4ddd-d394-63536db736ef"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "11999 3999 4001\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "N6P4Oi4_gCIY"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Setup printed text database"
      ],
      "metadata": {
        "id": "7GhX8dHINie8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "!pip install english-words\n",
        "from english_words import get_english_words_set\n",
        "!pip install essential_generators\n",
        "from essential_generators import DocumentGenerator\n",
        "\"\"\"\n",
        "!pip install nltk\n",
        "import nltk\n",
        "nltk.download('words')\n",
        "from nltk.corpus import words\n",
        "from random import sample"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vx7zTgfRUubN",
        "outputId": "90f8f4e8-a548-4f43-e559-0100669f0975"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: nltk in /usr/local/lib/python3.9/dist-packages (3.8.1)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.9/dist-packages (from nltk) (1.2.0)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.9/dist-packages (from nltk) (2022.10.31)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.9/dist-packages (from nltk) (4.65.0)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.9/dist-packages (from nltk) (8.1.3)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package words to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/words.zip.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# https://stackoverflow.com/questions/55200307/generate-a-string-of-n-random-english-words-with-nltk-python\n",
        "def random_sentence(n):\n",
        "    return ' '.join(sample(words.words(), n)) # calling it too many times is  inefficient\n",
        "    # maybe better to use a prexisting word database (althougt not neeeded as sentences.txt has already been generated, and only once is required)"
      ],
      "metadata": {
        "id": "ucLi7wn9W6v2"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import random, shutil\n",
        "# GENERATE_SENTENCES = input(\"Do you want to generate sentences? (y/n)\") == \"y\"\n",
        "GENERATE_SENTENCES = False # already generated before. Hardcode to false to be safe\n",
        "PRINTED_TEXTS_PATH = DATASET_PATH # \"/content/drive/MyDrive/599 Project/custom-dataset-script\"\n",
        "class Sentence:\n",
        "  def __init__(self, tot_sentences=1, min_length=1, max_length=3):\n",
        "    assert 1 <= min_length <= max_length\n",
        "    # initialize sentences\n",
        "    self.tot_sentences = tot_sentences\n",
        "    self.sentences = [\"\"]*tot_sentences\n",
        "    for i in range(tot_sentences):\n",
        "      sentence_length = random.randint(min_length,max_length)\n",
        "      self.sentences[i] = random_sentence(sentence_length)\n",
        "  def __len__(self):\n",
        "    return self.tot_sentences\n",
        "  def __getitem__(self, i):\n",
        "    return self.sentences[i]\n",
        "  def __str__(self):\n",
        "    return str(self.sentences)\n",
        "\n",
        "if GENERATE_SENTENCES:\n",
        "  texts = Sentence(10000)\n",
        "  # texts = Sentence(100)\n",
        "  print(\"total sentences:\", len(texts))\n",
        "  # print(texts)\n",
        "\n",
        "  # export to file\n",
        "  with open('sentences.txt', 'w') as file:\n",
        "    for sentence in texts:\n",
        "      file.write(sentence+\"\\n\")\n",
        "  # copy file to drive\n",
        "  shutil.copy(\"sentences.txt\", PRINTED_TEXTS_PATH)\n",
        "\n",
        "# copy file from drive\n",
        "shutil.copy(os.path.join(DATASET_PATH, \"sentences.txt\"),\".\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "zTBXpH8dNkl_",
        "outputId": "e22bb3a3-4258-4608-a912-09e67b33cedd"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'./sentences.txt'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "with open('sentences.txt', 'r') as file:\n",
        "  texts = list(map(lambda t: t.rstrip('\\n'), file.readlines()))\n",
        "\n",
        "# split the texts into training, validation and test (60, 20, 20)\n",
        "from torch.utils.data import random_split\n",
        "\n",
        "train_size = int(0.6 * len(texts))\n",
        "val_size = int(0.2 * len(texts))\n",
        "test_size = len(texts) - train_size - val_size\n",
        "\n",
        "train_text_printed, val_text_printed, test_text_printed = random_split(texts, [train_size, val_size, test_size])\n",
        "print(len(train_text_printed),len(val_text_printed),len(test_text_printed) )"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9xH8YdAIZdCE",
        "outputId": "bfce27c8-da30-4973-d3da-28f9ee09b92a"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "6000 2000 2000\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# setup one font\n",
        "extract_zip_to(\"/content/drive/MyDrive/599 Project/custom-dataset-script/fonts/roboto.zip\", \"./fonts\")\n",
        "FONT_PATH =  \"./fonts/Roboto-Regular.ttf\"\n",
        "\n",
        "# GENERATE_PRINTED_IMAGES = input(\"Do you want to generate printed images? (y/n)\") == \"y\"\n",
        "GENERATE_PRINTED_IMAGES = False # from now and on, to directly extract instead of re-generating\n",
        "\n",
        "import shutil, os\n",
        "def zip_to(src, dest):\n",
        "  # create zip file with the nested folder as the folder `src`\n",
        "  # so that when extracting `src` folder is extracted instead of `src/*` files\n",
        "  shutil.make_archive(dest, 'zip', \".\", src)\n",
        "\n",
        "\n",
        "if GENERATE_PRINTED_IMAGES:\n",
        "  # generate printed images\n",
        "  generatePrintedImages(\"train_text_printed\", FONT_PATH, train_text_printed)\n",
        "  generatePrintedImages(\"val_text_printed\", FONT_PATH, val_text_printed)\n",
        "  generatePrintedImages(\"test_text_printed\", FONT_PATH, test_text_printed)\n",
        "\n",
        "  # zip and save all them on drive\n",
        "  zip_to(\"train_text_printed\", os.path.join(DATASET_PATH, \"train_text_printed\"))  \n",
        "  zip_to(\"val_text_printed\", os.path.join(DATASET_PATH, \"val_text_printed\"))\n",
        "  zip_to(\"test_text_printed\", os.path.join(DATASET_PATH, \"test_text_printed\"))\n",
        "\n",
        "# extract zip files from drive \n",
        "print(os.listdir(DATASET_PATH))\n",
        "extract_zip_to(os.path.join(DATASET_PATH, \"train_text_printed.zip\"), \".\")\n",
        "extract_zip_to(os.path.join(DATASET_PATH, \"val_text_printed.zip\"), \".\")\n",
        "extract_zip_to(os.path.join(DATASET_PATH, \"test_text_printed.zip\"), \".\")\n",
        "print(os.listdir(DATASET_PATH))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "A6kkZ0bNhYg8",
        "outputId": "69c5e879-2829-4f21-9627-722171f8c9e8"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Do you want to generate printed images? (y/n)y\n",
            "['handwritten-text-images-database.zip', 'handwritten-text-images-labels.csv', 'sample images', 'fonts', 'Custom Dataset.ipynb', 'sentences.txt', 'train_text_printed.zip', 'val_text_printed.zip', 'test_text_printed.zip']\n",
            "['handwritten-text-images-database.zip', 'handwritten-text-images-labels.csv', 'sample images', 'fonts', 'Custom Dataset.ipynb', 'sentences.txt', 'train_text_printed.zip', 'val_text_printed.zip', 'test_text_printed.zip']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "train_printed = TextDataset(\"train_text_printed\", \"train_text_printed/labels.csv\")\n",
        "val_printed = TextDataset(\"val_text_printed\", \"val_text_printed/labels.csv\")\n",
        "test_printed = TextDataset(\"test_text_printed\", \"test_text_printed/labels.csv\")\n",
        "print(len(train_printed), len(val_printed), len(test_printed))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OnioCAtrgA_V",
        "outputId": "e4618d61-b068-484b-96ac-2bd6bef58daa"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "6000 2000 2000\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Create custom dataset"
      ],
      "metadata": {
        "id": "evyqvIvHNn_g"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "6L5LN_SxNpt6"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Model"
      ],
      "metadata": {
        "id": "7pwxGX4qNsgC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## model setup"
      ],
      "metadata": {
        "id": "YNef4J-lNwVB"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "spEtzZfzNvya"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## model training"
      ],
      "metadata": {
        "id": "J7LSP8AaNyoT"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "9UJY_081Nzhv"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## model evaluation"
      ],
      "metadata": {
        "id": "SW9rgkuHN0Cp"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Xwecs5M3N1Ug"
      },
      "execution_count": 11,
      "outputs": []
    }
  ]
}